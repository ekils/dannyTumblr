#coding=utf-8
from dannyTumblr import *
import re
from bs4 import BeautifulSoup
import requests
import urllib.request
import os
import sys
import time
import progressbar
from dannyTumblr.Login import Log
from dannyTumblr.DataBase import Mysql_data
from dannyTumblr.Dataplot import DataPlot



class Tumblr:
    def __init__(self):
        self.ori_path= os.getcwd()
        print(self.ori_path)
        ll= Log()
        global driver
        driver = ll.Login()
        # å‘¼å«sql:
        self.mysqldata = Mysql_data()


    def parse(self):
        info= input(' << è¼¸å…¥è©²å¸³è™Ÿæˆ–ç¶²å€ï¼š >> \n')
        info_all = info
        pat = re.compile('(?:http|https)://.*.tumblr.com')   #   (?: A | B  ) å–ï¼¡æˆ–ï¼¢çš„æ­£å‰‡è¡¨é”å¼ï¼Œå¦‚æœä¸åŠ ï¼Ÿï¼š æœƒåªé¡¯ç¤ºé—œéµå­—ï¼¡æˆ–ï¼¢ è€Œä¸æ˜¯è¦çš„å…¨éƒ¨å­—ä¸²
        pat_post= re.compile('.*/post/.*')

    # default variables:
        types = 'full-page'
        page_number='1'
        information=''
    # çµ¦ç¶²å€
        if pat_post.search(info):
            pat_list= re.split('.tumblr.com/post/|/',info)
            if pat_list[1]=='':
                info=pat_list[2]
            else:
                info=pat_list[0]
            print('è¼¸å…¥çš„å¸³è™Ÿæ˜¯ï¼š{}'.format(info))
            types='post-page'
    # çµ¦å¸³è™Ÿ
        elif pat.search(info):
            print('è¼¸å…¥çš„å¸³è™Ÿæ˜¯ï¼š{}'.format(info))
        # è®€å–å…ˆå‰é æ•¸è³‡æ–™ï¼š
            self.mysqldata.Page_mksure(info)
            page_number = input('<<çµ¦é æ•¸ç¯„åœï¼špage,page >>\n')
            info= pat.search(info).group()
            info= info.split('.tumblr.com')[0]
            info = re.split('(?:http://|https://)',info)[1]
        # æ›´æ–°é æ•¸è³‡æ–™ï¼š
            self.mysqldata.PG(info, page_number)
        else:
            print('è¼¸å…¥çš„å¸³è™Ÿæ˜¯ï¼š{}'.format(info))
        # è®€å–å…ˆå‰é æ•¸è³‡æ–™ï¼š
            self.mysqldata.Page_mksure(info)
            page_number = input('<<çµ¦é æ•¸ç¯„åœï¼špage,page >>\n')
            info=info
        # æ›´æ–°é æ•¸è³‡æ–™ï¼š
            self.mysqldata.PG(info, page_number)


        pages=[]
        if types == 'full-page':
            tag_info = input('paste tags or input 0 for none :')
            if tag_info == '0':
                information = 'http://' + info + '.tumblr.com/page/' + page_number
            else:
                information = tag_info + '/page/' + page_number

            if len(page_number.split(',')) > 1:
                # é æ•¸ç¯„åœï¼š
                pages = [int(page_number.split(',')[0]),int(page_number.split(',')[1])]
            else:
                # å–®ä¸€é ï¼š
                pages = [int(page_number)]


        elif types == 'post-page':
            information = info_all

        # Crawler:
        start = time.time()
        try:
            driver.set_page_load_timeout(30)
            driver.get(information)
        except :
            print(' \n~~~~é€£ç·šå¤ªä¹…å•¦~~~~\n ')
            sys.exit()

        soup = BeautifulSoup(driver.page_source, "html.parser")
        soup_confirm = str(soup)

        print('types:{}'.format(types))
        end = time.time()
        count = round((end-start),3)
        print('å•Ÿå‹•ç€è¦½èµ·è€—æ™‚ï¼š{} ç§’'.format(count))


        # å…ˆåˆ¤æ–·ä¾†æºæ˜¯å¦éŒ¯èª¤ï¼š
        if soup_confirm == '<html><head></head><body></body></html>':   # ç¶²å€keyéŒ¯
            print('404 Not Found1')
        elif str(soup.find('title')) == '<title>Not found.</title>':      # æŸ¥ç„¡æ­¤å¸³è™Ÿ
            print('404 Not Found2')
        #  http://ekilsekilsekilsdaliuchiachum.tumblr.com

        else:
            self.download(info,info_all,types,information,pages)  # info_all æ”¹ç‚ºlist


    def download(self,info,info_all,types,information,pages):
        print('')
        print('<<<<<<<< Loading  >>>>>>>>')
        print('')
        list_photo = []
        if types== 'post-page':
            urllist = [info_all]
            self.download_post_page(info,urllist)

        elif types == 'full-page':
            self.download_post_page(info,self.download_full_page(info,information,pages))


    def download_post_page(self,info,urls):

        # è¨­å®šè³‡æ–™å¤¾å¤§å°ï¼š
        Check = self.mysqldata.Default_for_folder_size(info)

        # æ›´æ–°è³‡æ–™å¤¾æª”æ¡ˆå¤§å°ï¼š
        folder_size = self.file_size(info)
        if Check == False:
            self.mysqldata.update_folder_size(info, self.file_size(info))

        # é€²å…¥download å›åœˆï¼š
        pat_jpg = re.compile('(?:http|https)://78.media.tumblr.com/.*/tumblr_.*?\.(?:png|jpg|gif)')  # ? åŒ¹é…0æ¬¡æˆ–ä¸€æ¬¡
        # print(urls)
        list_photo = []
        minus_article_count = 0 # æ–‡ç« è¨ˆæ•¸å™¨

        for adress in urls:
            bool_for_tomeout = True

            while bool_for_tomeout == True:
                try:
                    driver.get(adress)   # ä¹‹å¾ŒåŠ å€‹try ä»¥é˜²ä¹‹å¾Œç¶²è·¯å•é¡Œcrash
                    soups = BeautifulSoup(driver.page_source, "html.parser")

                    # æ‰¾tagè£¡çš„meta å’Œ imgï¼š
                    photo_meta = soups.find_all('meta')  # åœ–è—åœ¨ content è£¡é¢
                    photo_imgsrc = soups.find_all('img')  # åœ–å­˜åœ¨tag img è£¡çš„ src

                    # æŠŠæ‰¾åˆ°çš„url ä¸Ÿé€²istè£¡å»ï¼š
                    photo_meta_url_list = [pat_jpg.search(str(i)).group() for i in photo_meta if pat_jpg.search(str(i))]
                    photo_imgsrc_ = [i.get('src') for i in photo_imgsrc]
                    photo_imgsrc_list = [pat_jpg.search(str(i)).group() for i in photo_imgsrc_ if pat_jpg.search(str(i))]


                    # print('photo_meta_url_list :{}'.format(photo_meta_url_list))
                    # print ('')
                    # print('photo_imgsrc_list :{}'.format(photo_imgsrc_list))
                    # print('')
                    #pat_compare = re.compile('(?<=_)\d+\.(?:png|jpg|gif)')  # _abc7abc    å‘å¾Œæ–·è¨€ï¼š(?<=_)abc  ï¼šæ‰¾abcé–‹é ­çš„å­—ä¸²ï¼Œä¸”è©²å­—ä¸²å‰é¢æœ‰ï¼¿ ï¼Œæ‰€ä»¥åªæœƒæ‰¾åˆ°ï¼šabc

                    pat1 = re.compile('tumblr.*\.(?:png|jpg|gif)') # åªè£å‡º tumblr_äº‚ç¢¼ï¼¿è§£æåº¦.jpg:
                    pat2 = re.compile('(?<=_).*\.(?:png|jpg|gif)') # å†è£å‡º äº‚ç¢¼ï¼¿è§£æåº¦.jpg
                    pat3 = re.compile('(.*)_(\d+)')    # æœ€å¾ŒæŠŠäº‚ç¢¼è·Ÿè§£æåº¦åˆ†å‡º

                    # tumblr:
                    tumblr_first_stage_url = re.findall('"https://www.tumblr.com/video/.*/"',
                                                        str(soups))  # ç¬¬ä¸€éšæ®µï¼šå–å‡ºè½‰è¼‰ç¶²å€ï¼Œä½†é€™é‚„ä¸æ˜¯å½±ç‰‡ç¶²å€
                    # Instgram;
                    pat_insta = re.compile('https://www.instagram.com/')



                    # å¦‚æœåœ¨ meta,img éƒ½æ‰¾åˆ° å°±çœ‹èª°çš„è§£æåº¦é«˜ ï¼š
                    if photo_meta_url_list != [] and photo_imgsrc_list != []:

                        # meta:
                        meta_list_get_from_return = self.meta_solution(photo_meta_url_list, pat1, pat2, pat3)    #[0]:resolution ; [1]: url_list ; [2]: smallest resolution position

                        # img:
                        imgsrc_list_get_from_return = self.imgsrc_solution(photo_imgsrc_list, pat1, pat2, pat3)   #[0]:resolution ; [1]: url_list ; [2]: smallest resolution position


                        # ç¸½è§£æåº¦ï¼š
                        meta_all_resolution = 0
                        for i in meta_list_get_from_return[0]:
                            meta_all_resolution = meta_all_resolution + int(i)

                        imgsrc_all_resolution = 0
                        for i in imgsrc_list_get_from_return[0]:
                            imgsrc_all_resolution = imgsrc_all_resolution + int(i)


                        # æ¯”è¼ƒ @1 ~ @4ï¼š
                        if meta_list_get_from_return[2] == [] and imgsrc_list_get_from_return[2] != []: # @1
                            list_photo= meta_list_get_from_return[1]
                            #print('@1 :list_photo= photo_imgsrc_list:{}'.format(list_photo))
                            print('@1')
                        elif meta_list_get_from_return[2] == [] and imgsrc_list_get_from_return[2] !=[]: #@2
                            list_photo= imgsrc_list_get_from_return[1]
                            #print('@2 :list_photo= photo_imgsrc_list:{}'.format(list_photo))
                            print('@2')
                        else:

                            if meta_all_resolution >= imgsrc_all_resolution:
                                list_photo= meta_list_get_from_return[1]
                                #print('@3: list_photo= photo_meta_url_list:{}'.format(list_photo)) #@3
                                print('@3')
                            elif meta_all_resolution < imgsrc_all_resolution:
                                list_photo = imgsrc_list_get_from_return[1]
                                #print('@4: list_photo= photo_imgsrc_list:{}'.format(list_photo)) #@4
                                print('@4')

                        if tumblr_first_stage_url != []:
                            # print(vodeo_first_stage_url)
                            print('Tumblr Video \n')
                            self.savevideo_tumblr(info, tumblr_first_stage_url)  # ç¬¬äºŒéšæ®µä¸Ÿå…¥åˆ†æ

                        elif pat_insta.search(str(soups)):
                            # Instgram;
                            # https://stem-stims.tumblr.com/
                            print('Instagram Video \n')
                            ig_url = (soups.find_all('iframe'))
                            self.savevideo_instagram(info, ig_url)


                    # ä¸ç„¶å°±çœ‹æ‰¾åˆ°èª°çš„æ ¼å¼ï¼š
                    else:
                        if photo_meta_url_list != []:
                            print('metaæœ‰æ‰¾åˆ°åœ–ç‰‡æª”')
                            print('meta:{}'.format(photo_meta_url_list))
                            # meta:
                            meta_list_get_from_return = self.meta_solution(photo_meta_url_list, pat1, pat2,pat3)  # [0]:resolution ; [1]: url_list ; [2]: smallest resolution position
                            list_photo = meta_list_get_from_return[1]

                            if tumblr_first_stage_url != []:
                                # print(vodeo_first_stage_url)
                                print('Tumblr Video \n')
                                self.savevideo_tumblr(info, tumblr_first_stage_url)  # ç¬¬äºŒéšæ®µä¸Ÿå…¥åˆ†æ

                            elif pat_insta.search(str(soups)):
                                # Instgram;
                                # https://stem-stims.tumblr.com/
                                print('Instagram Video \n')
                                ig_url = (soups.find_all('iframe'))
                                self.savevideo_instagram(info, ig_url)

                        elif photo_imgsrc_list != []:
                            print('imgsrcæœ‰æ‰¾åˆ°åœ–ç‰‡æª”')
                            photo_imgsrc_list = [pat_jpg.search(i).group() for i in photo_imgsrc_list if pat_jpg.search(i)]
                            print('src:{}'.format(photo_imgsrc_list))
                            # img:
                            imgsrc_list_get_from_return = self.imgsrc_solution(photo_imgsrc_list, pat1, pat2,pat3)  # [0]:resolution ; [1]: url_list ; [2]: smallest resolution position
                            list_photo = imgsrc_list_get_from_return[1]

                            if tumblr_first_stage_url != []:
                                # print(vodeo_first_stage_url)
                                print('Tumblr Video \n')
                                self.savevideo_tumblr(info, tumblr_first_stage_url)  # ç¬¬äºŒéšæ®µä¸Ÿå…¥åˆ†æ

                            elif pat_insta.search(str(soups)):
                                # Instgram;
                                # https://stem-stims.tumblr.com/
                                print('Instagram Video \n')
                                ig_url = (soups.find_all('iframe'))
                                self.savevideo_instagram(info, ig_url)

                        else:

                            if tumblr_first_stage_url!=[]:
                                #print(vodeo_first_stage_url)
                                print('Tumblr Video \n')
                                self.savevideo_tumblr(info,tumblr_first_stage_url)  # ç¬¬äºŒéšæ®µä¸Ÿå…¥åˆ†æ

                            elif pat_insta.search(str(soups)):
                                # Instgram;
                                # https://stem-stims.tumblr.com/
                                print('Instagram Video \n')
                                ig_url = (soups.find_all('iframe'))
                                self.savevideo_instagram(info,ig_url)

                            else:
                                print('  !!  å¯èƒ½è©²Postæ²’æ±è¥¿æŠ“  !!  ')
                                print('')
                    bool_for_tomeout = False

                    if len(list_photo) != 0:
                        self.savephoto(info,list_photo)
                        list_photo = []
                    minus_article_count += 1
                    try:
                        len(self.all_articles)
                        print('å‰©ä¸‹ {} posts'.format(int(len(self.all_articles)) - int(minus_article_count)))
                    except:
                        break
                except:
                    bool_for_tomeout = True
                    print('æ‰ç·š é‡æ–°å•Ÿå‹•è™›æ“¬ç€è¦½å™¨è©¦è©¦')

        after_size = self.file_size(info)
        self.mysqldata.DL(info,folder_size,after_size)
        T= True
        while T==True:
            tof = input('ä¸‹è¼‰å®Œç•¢ï¼Œæ˜¯å¦éœ€è¦æŸ¥çœ‹ Dataåº«çš„æ•¸æ“šï¼Ÿ 1.Yes 2.No : ')

            if tof == '1':
                T = True
                # å‘¼å« dataplot:
                self.myplot = DataPlot()
                keyin = input('1. æŸ¥çœ‹Loginè³‡æ–™ 2. æŸ¥çœ‹æ¯æ—¥æµé‡ 3. æŸ¥çœ‹æ¯æœˆæµé‡  4. é›¢é–‹ : ')
                if keyin == '1':
                    self.myplot.LoginPlot()
                elif keyin == '2':
                    self.myplot.Month_PerDaily_Stream_Plot()
                elif keyin == '3':
                    self.myplot.Year_PerMonthly_Stream_Plot()
                else:
                    print('Quit')
                    T = False
            elif tof == '2':
                T = False
            else:
                T = True


    def download_full_page(self, info, information,pages):  # åœ¨full-page æ‰¾åˆ°æ¯å‰‡poæ–‡çš„url æœ€å¾Œæœƒä¸Ÿå›åˆ° post-page å»è¼‰è³‡æ–™

        # pat_information_cut = re.compile('http://.*.tumblr.com/page/' )
        pat_information_cut = re.compile('http://.*./page/')
        information_cut_for_crawler = pat_information_cut.search(information).group()
        article_post_filelist_for_loop = []
        #print('pages:{}'.format(pages))
        if len(pages) == 1:
            pages.append(pages[0])

        for pg in range(pages[0],pages[1]+1):
            bool_for_now_loading = True
            while bool_for_now_loading == True:
                try:
                    # Crawler for list pages:
                    driver.implicitly_wait(20)  # seconds  æ¯å‰‡è®€å–è¨­å®šç­‰å¾…æœ€å¤š20ç§’ ä¸è¨­å®šè®€åˆ°å¾Œé¢æœƒå ±éŒ¯
                    driver.get(information_cut_for_crawler+str(pg))
                    soup = BeautifulSoup(driver.page_source, "html.parser")

                    print('æ­£åœ¨è®€å–ç¬¬{}é  ğŸ’¬ '.format(pg))

                    if soup.find_all('article'):
                        print('##### Situation 1: Postæ–‡ç« ç·¨è™Ÿ è—åœ¨article  #####')

                        article_post_num = []
                        article_post = soup.find_all('article')
                        for i in article_post:
                            if i.get('id'):
                                notcut = i.get('id')
                                cut = notcut.split('post-')[1]
                                article_post_num.append(cut)
                            elif i.get('data-post-id'):
                                article_post_num.append(i.get('data-post-id'))
                            else:
                                if soup.find('div'):
                                    print('##### Situation ä¾‹å¤–: Postæ–‡ç« ç·¨è™Ÿ å…¶å¯¦è—åœ¨div #####')

                                    article_post = soup.find_all('div')
                                    article_post_num = [i.get('data-post-id') for i in article_post]
                                    article_post_num = [i for i in article_post_num if i != None]
                                else:
                                    print('article fond id wrong')
                        article_post_filelist = ['https://' + info + '.tumblr.com/post/' + i for i in article_post_num]
                        #print('æ‰€æœ‰ç¶²å€:{}'.format(article_post_filelist))
                        print('')
                        #return article_post_filelist

                    elif soup.find('div'):
                        print('##### Situation 2: Postæ–‡ç« ç·¨è™Ÿ è—åœ¨div #####')
                        article_post = soup.find_all('div')
                        article_post_num = [i.get('data-post-id') for i in article_post]
                        article_post_num = [i for i in article_post_num if i != None]
                        article_post_filelist = ['https://' + info + '.tumblr.com/post/' + i for i in article_post_num]
                        if article_post_filelist != []:
                            print('æ‰€æœ‰ç¶²å€:{}'.format(article_post_filelist))
                            print(len(article_post_filelist))
                            #return article_post_filelist
                        else:
                            article_post_num = []
                            article_post_filelist = []
                            print('New éš±è—ä½ç½®ï¼¿new version: 2018/01/29')
                            print('â†“â†“â†“â†“â†“â†“â†“')
                            try:
                                again= soup.find_all('a')
                                # print(again)
                                for i in again:
                                    g= i.get('href')
                                    article_post_num.append(g)
                                # print(article_post_num)

                                pat20180129 = re.compile('.*/post/.*')
                                for i in article_post_num:
                                    if pat20180129.search(i) :
                                        temp_c= pat20180129.search(i).group()
                                        # print(temp_c)
                                        article_post_filelist.append(temp_c)
                                print(article_post_filelist)

                            except:
                                print(damn)

                            print('â†‘â†‘â†‘â†‘â†‘â†‘â†‘')
                    else:
                        print('##### Situation else #####')

                    [article_post_filelist_for_loop.append(af) for af in article_post_filelist]
                    bool_for_now_loading = False
                except:
                    print('æ­£åœ¨è®€å– ä½†æ˜¯æ‰ç·šæ­£åœ¨é‡æ–°å•Ÿå‹•è™›æ“¬ç€è¦½å™¨')
                    bool_for_now_loading = True

        print('æ‰€æœ‰ç¶²å€å…±{}å‰‡'.format(len(article_post_filelist_for_loop)))
        self.all_articles = article_post_filelist_for_loop
        print('ç¬¬{}~{}é  çš„æ‰€æœ‰ç¶²å€:{}'.format(pages[0],pages[1],article_post_filelist_for_loop))
        return article_post_filelist_for_loop


    def find_same_garbled_position(self,l): # l: list

        aa = [[i for i, v in enumerate(l) if v == x] for x in l]
        ap = [i for i in aa if len(i) > 1]
        ape = [str(i) for i in ap]
        l2 = []
        l3 = []
        [l2.append(i) for i in ape if not i in l2]
        for i in l2:
            yup = re.split(']|\[|,', i)
            a = [ii for ii in yup if ii != '']
            l3.append(a)
        return l3
        #print(l3)


    def meta_solution(self,photo_meta_url_list,pat1,pat2,pat3):
        # meta:
        # åªè£å‡º tumblr_äº‚ç¢¼ï¼¿è§£æåº¦.jpg:
        temp_meta1 = [pat1.search(i).group() for i in photo_meta_url_list]
        # å†è£å‡º äº‚ç¢¼ï¼¿è§£æåº¦.jpg
        temp_meta2 = [pat2.search(i).group() for i in temp_meta1]

        meta_garbled = [pat3.search(i).group(1) for i in temp_meta2]  # äº‚ç¢¼
        meta_resolution = [pat3.search(i).group(2) for i in temp_meta2]  # è§£æåº¦

        # æ‰¾é‡è¤‡äº‚ç¢¼çš„ä½ç½®ï¼š
        find_same_garbled_position = self.find_same_garbled_position(meta_garbled)

        not_just_zero = 10000  # å¯«å€‹å¾ˆå¤§çš„è§£æåº¦
        smallest_resolution_position = []
        if find_same_garbled_position == []:
            pass
        else:
            print('meta find_same_garbled_position:{}'.format(find_same_garbled_position))
            for ii in find_same_garbled_position:
                ipis = 0
                for ipis in ii:
                    if int(meta_resolution[int(ipis)]) < not_just_zero:
                        not_just_zero = int(meta_resolution[int(ipis)])

                smallest_resolution_position.append(int(ipis))
                print('meta Not just zeroæœ€ä½è§£æåº¦:{}'.format(not_just_zero))
            print('meta æœ€ä½è§£æåº¦ä½ç½®:{}'.format(smallest_resolution_position))

            meta_resolution = [ii for i, ii in enumerate(meta_resolution) if i != smallest_resolution_position]

            photo_meta_url_list = [ii for i, ii in enumerate(photo_meta_url_list) if i not in smallest_resolution_position]
            print('meta åˆªé™¤é‡è¤‡çš„url listå¾Œ:{}'.format(photo_meta_url_list))

        meta_list_for_return=[meta_resolution,photo_meta_url_list,smallest_resolution_position]
        return meta_list_for_return


    def imgsrc_solution(self,photo_imgsrc_list,pat1,pat2,pat3):
        # img:
        # åªè£å‡º tumblr_äº‚ç¢¼ï¼¿è§£æåº¦.jpg:
        temp_src1 = [pat1.search(i).group() for i in photo_imgsrc_list]
        # å†è£å‡º äº‚ç¢¼ï¼¿è§£æåº¦.jpg
        temp_src2 = [pat2.search(i).group() for i in temp_src1]

        imgsrc_garbled = [pat3.search(i).group(1) for i in temp_src2]  # äº‚ç¢¼
        imgsrc_resolution = [pat3.search(i).group(2) for i in temp_src2]  # è§£æåº¦

        # æ‰¾é‡è¤‡äº‚ç¢¼çš„ä½ç½®ï¼š
        find_same_garbled_position2 = self.find_same_garbled_position(imgsrc_garbled)

        not_just_zero2 = 10000
        smallest_resolution_position2 = []
        if find_same_garbled_position2 == []:
            pass
        else:
            print('imgsrc find_same_garbled_position2:{}'.format(find_same_garbled_position2))
            for ii in find_same_garbled_position2:
                ipis=0
                for i in ii:
                    if int(imgsrc_resolution[int(ipis)]) < not_just_zero2:
                        not_just_zero2 = int(imgsrc_resolution[int(ipis)])
                smallest_resolution_position2.append(int(ipis))
                print('imgsrc Not just zero2æœ€ä½è§£æåº¦:{}'.format(not_just_zero2))
            print('imgsrc æœ€ä½è§£æåº¦ä½ç½®:{}'.format(smallest_resolution_position2))

            imgsrc_resolution = [ii for i, ii in enumerate(imgsrc_resolution) if i != smallest_resolution_position2]

            photo_imgsrc_list = [ii for i, ii in enumerate(photo_imgsrc_list) if i not in smallest_resolution_position2]
            print('imgsrc åˆªé™¤é‡è¤‡çš„url listå¾Œ:{}'.format(photo_imgsrc_list))

        imgsrc_list_for_return =[imgsrc_resolution,photo_imgsrc_list,smallest_resolution_position2]
        return imgsrc_list_for_return


    def savephoto(self,info,list_photo):

        # <<<<<PHOTO>>>>>>>
        if list_photo != []:
            if not os.path.exists(self.ori_path + '/' + 'dannyTumblr' + '/' + 'Download' + '/' + info+ '/' + 'Photos'):
                os.makedirs(self.ori_path + '/' + 'dannyTumblr' + '/' + 'Download' + '/' + info+ '/'  + 'Photos')
            os.chdir(self.ori_path + '/' + 'dannyTumblr' + '/' + 'Download' + '/' + info+ '/'  + 'Photos')
            pat_photoname = re.compile('tumblr_.*?\.(png|jpg|gif)')
            for index, i in enumerate(list_photo):
                url_photo = urllib.request.urlopen(i).read()

                try:
                    tumblr_name = pat_photoname.search(i).group()
                    garbled_name=re.split('_|\.', tumblr_name)

                    print('file_name:{}'.format(tumblr_name))

                    if os.path.exists( self.ori_path + '/' + 'dannyTumblr' + '/' + 'Download' + '/' + info+ '/'  + 'Photos' + '/'+ tumblr_name):
                        print('Already Downloaded :(')

                    else:
                        print('New :)')
                        with open(tumblr_name, 'wb') as pics:
                            pics.write(url_photo)
                            pics.close()
                except:
                    print('Write Wrong')
                    pass
            print('')
            print('CYCLE DONE_ "çˆ¬å®Œä¸€å‰‡è²¼æ–‡äº†" _CYCLE DONE ')
            os.chdir(self.ori_path)


    def save_continue_progress(self,info,videofilename,url):

        dl = True
        path01 = os.getcwd()
        while_counter = 0
        slice = 1024 * 1  # è®€å–å½±ç‰‡ä¸²æµå€å¡Šå¤§å°

        while dl == True:
            iterator = 0
            clist = []
            piece_count = 0
            print('è®€å–è³‡æ–™ä¸­.........')

            with open(videofilename + '.mp4', 'wb')as f:
                try:
                    print('start')

                    r = requests.get(url, stream=True,timeout=15)
                    file_size = int(r.headers['Content-Length'])
                    size = str(file_size)
                    piece = int(int(size) / 1024)
                    print('piece:{}'.format(piece))

                    bar = progressbar.ProgressBar(maxval=file_size, widgets=[progressbar.Percentage(), ' : ',
                                                                             progressbar.Counter(), ' of ',
                                                                             progressbar.FormatCustomText(size),
                                                                             ' bytes',
                                                                             ' [', progressbar.AdaptiveTransferSpeed(),
                                                                             '] ',
                                                                             # progressbar.ETA(),
                                                                             # ' [',progressbar.Timer(), '] ',
                                                                             progressbar.Bar(marker='â– '),
                                                                             ]).start()

                    for chunk in r.iter_content(chunk_size=slice):
                        try:
                            def gen():
                                yield chunk

                            ge = gen()
                            if len(chunk) == 1024:
                                f.write(ge.__next__())

                                clist.append(len(chunk))

                                # f.write(chunk)
                                iterator += slice
                                bar.update(iterator)
                                piece_count += 1
                                if int(piece_count) == int(piece):
                                    dl = False
                                    break
                            elif int(piece_count) == int(piece):
                                dl = False
                                break
                            else:
                                print('piece_count :{}'.format(piece_count))
                                print(' ä¸åˆ° 1MB é‡æŠ“')
                                f.write(ge.__next__())

                        except:
                            print('ä¾†æ­¤èµ°ä¸€é­çš„ é»‘äººå•è™Ÿ')


                except requests.exceptions.ConnectionError:
                    print('\n  æ‰ç·šæ‹‰ ')
                    dl = True
                    while_counter += 1
                    if while_counter == 10:
                        print('Give up...')
                        dl = False
                except:
                    print('é»‘äººå•è™Ÿ')

                finally:
                    if dl:
                        print(piece_count)
                        print(' ======Disconnect ======')
                        print('  Close file ;(')
                        os.remove(self.ori_path + '/' + 'dannyTumblr' + '/' + 'Download' + '/' + info+ '/' + 'Videos')
                    else:
                        print('  Download Done')
                        print('  Close file :>')
                        # print(clist)
                        for i in clist:
                            if i < 1024:
                                print(i)
                        print('å½±ç‰‡ç¸½:{}å­—ä¸²'.format(len(clist)))
                    f.close()
        return

    # Tumblr:
    def savevideo_tumblr(self,info,tumblr_first_stage_url):

        if not os.path.exists(self.ori_path + '/' + 'dannyTumblr' + '/' + 'Download' + '/' + info+ '/' + 'Videos'):
            os.makedirs(self.ori_path + '/' + 'dannyTumblr' + '/' + 'Download' + '/' + info+ '/' + 'Videos')
        os.chdir(self.ori_path + '/' + 'dannyTumblr' + '/' + 'Download' + '/' + info+ '/' + 'Videos')


        pat_tumblr = re.compile('".*"')
        pat_tumblr_for_temp = re.compile('tumblr_.*')
        pat_tumblr_video_r1 = re.compile('.*r1')
        pat_tumblr_video_r2 = re.compile('.*r2')

        for i in tumblr_first_stage_url:
            dic_tumblr = (pat_tumblr.search(i).group()).split('"')[1]
            print(dic_tumblr)
            # ç¬¬äºŒéšæ®µï¼š å°‡ç¬¬ä¸€éšæ®µçš„ç¶²å€å†é€²è¡Œåˆ†æ
            driver.get(dic_tumblr)
            soup = BeautifulSoup(driver.page_source, "html.parser")


            temp_url = soup.find('video').get('poster')  # ç¬¬äºŒéšæ®µï¼š åœ¨chrome çš„media è§€å¯Ÿå¾Œæœƒç™¼ç¾ï¼Œå½±ç‰‡çš„è³‡è¨Šæœƒåœ¨é€™è£¡å‡ºç¾
            url = pat_tumblr_for_temp.search(temp_url).group()
            print('url:{}'.format(url))

            if pat_tumblr_video_r1.search(url) or pat_tumblr_video_r2.search(url):
                if len(url.split('_smart1.jpg')) > 1:
                    url = url.split('_smart1.jpg')
                else:
                    url = url.split('_frame1.jpg')

            else:
                temp_url = soup.find('source').get('src')  # ç¬¬äºŒéšæ®µï¼š åœ¨chrome çš„media è§€å¯Ÿå¾Œæœƒç™¼ç¾ï¼Œå½±ç‰‡çš„è³‡è¨Šæœƒåœ¨é€™è£¡å‡ºç¾
                url = pat_tumblr_for_temp.search(temp_url).group()
                url = url.split('/')  # æœ‰æ™‚å…¶ç¶²å€æœƒå¤šå€‹ /æ•¸å­— ï¼Œæ‰€ä»¥è¦è¸¢æ‰ï¼Œæˆ–è½‰ _  ä¸ç„¶å½±ç‰‡ç¶²å€æœƒéŒ¯

            tumblr_url = 'https://vt.media.tumblr.com/' + url[0] + '.mp4'  # .mp4 file
            print(tumblr_url)

            videofilename = url[0]

            if os.path.exists(self.ori_path + '/' + 'dannyTumblr' + '/' + 'Download' + '/' + info+ '/' + 'Videos'+ '/' + videofilename + '.mp4'):
                    print('Video Already Downloaded x(')
            else:
                print(' NEW :P')
                start = time.time()
                # try:
                #     data = urllib.request.urlopen(tumblr_url).read()
                # except:
                #     continue
                # with open(url[0] + '.mp4','wb') as video:
                #     video.write(data)
                #     video.close()

                self.save_continue_progress(info,videofilename,tumblr_url)

                end = time.time()
                count_time = round((end - start), 3)
                print('Finish in ï¼š{} sec'.format(count_time))



        print('CYCLE DONE_ "çˆ¬å®Œä¸€å‰‡è²¼æ–‡äº†" _CYCLE DONE ')
        os.chdir(self.ori_path)

    # Instagram
    def savevideo_instagram(self,info,ig_url):

        if not os.path.exists(self.ori_path + '/' + 'dannyTumblr' + '/' + 'Download' + '/' + info+ '/' + 'Videos'):
            os.makedirs(self.ori_path + '/' + 'dannyTumblr' + '/' + 'Download' + '/' + info+ '/' + 'Videos')
        os.chdir(self.ori_path + '/' + 'dannyTumblr' + '/' + 'Download' + '/' + info+ '/' + 'Videos')


        pat_insta = re.compile('https://www.instagram.com/')
        pat_insta_videourl = re.compile(' ".*"')
        filename=''
        for index, content in enumerate(ig_url):
            # print(content)
            if pat_insta.search(str(content.get('src'))):
                # print (  str(content.get('src'))  )
                driver.get(content.get('src'))  # åˆ©ç”¨çˆ¬åˆ°çš„igç¶²å€å» instagramå€‹äººç¶²ç«™å†çˆ¬å…¶åŸå§‹å½±ç‰‡æª”
                soup = BeautifulSoup(driver.page_source, "html.parser")
                ss = soup.find_all('script')  # find æ˜¯æ‰¾tagçš„é ­
                # print(ss[1])
                # ssæ‰¾åˆ°çš„script list ç¬¬0å€‹ ä¸æ˜¯æˆ‘è¦çš„è³‡è¨Šï¼Œè€Œç¬¬ä¸€å€‹è£¡é¢æœ‰videoçš„è³‡è¨Šï¼Œæ‰€ä»¥å¾ç¬¬ä¸€å€‹ç¯©é¸ï¼š
                dic = re.findall(' "video_url": ".*?", ', str(ss[1]))
                insta_url = str(pat_insta_videourl.search(dic[0]).group())
                insta_url = insta_url.split('"')
                print(insta_url[3])  # æ‰¾åˆ° .mp4 file
                try:
                    filename = insta_url[3].split('https://instagram.ftpe7-1.fna.fbcdn.net/t50.2886-16/')[1]
                except:
                    print('å¯èƒ½æ˜¯IGçš„å½±ç‰‡æª”ç¶²å€ç‰ˆæœ¬åˆæ”¹äº† å°è‡´ä¸‹è¼‰æœ‰èª¤')
                #print(filename)

                videofilename = filename.split('.mp4')[0]

                if os.path.exists(self.ori_path + '/' + 'dannyTumblr' + '/' + 'Download' + '/' + info+ '/' + 'Videos' + '/' + filename):
                    print('Video Already Downloaded x(')
                else:
                    print(' NEW :P')
                    start = time.time()

                    self.save_continue_progress(info, videofilename, insta_url[3])

                    # data_insta = urllib.request.urlopen(insta_url[3]).read()
                    # with open(filename, 'wb') as video_insta:
                    #     video_insta.write(data_insta)
                    #     video_insta.close()
                    end = time.time()
                    count_time = round((end - start), 3)
                    print('Finish in ï¼š{} sec'.format(count_time))
            else:
                print('instagram wrong? å¾ˆæœ‰å¯èƒ½æ˜¯IGçš„å½±ç‰‡æª”ç¶²å€ç‰ˆæœ¬åˆæ”¹äº† ')
                break
        print('CYCLE DONE_ "çˆ¬å®Œä¸€å‰‡è²¼æ–‡äº†" _CYCLE DONE ')
        os.chdir(self.ori_path)

        # # Youtube :ï¼ˆå°šæœªç¢°åˆ°tumblrè¦ç”¨youtube,æ‰€ä»¥æš«æ™‚ä¸å¯«é€²å»  ï¼‰
        # import json
        # req = requests.get(input("è¼¸å…¥ç¶²å€ï¼š"))
        # # å‘ä¸€ç¶²é åšè«‹æ±‚
        # #print (req.text)
        #
        # dic = re.findall("ytplayer.config = ({.*?});", req.text)
        # # æ“·å–ç¶²é ä¾†æºä¸­çš„å­—å…¸
        #
        # js = json.loads(dic[0])
        # # json ç·¨è­¯
        # #print (js)
        # urls = urllib.parse.parse_qs(js['args']['url_encoded_fmt_stream_map'])
        # #print (urls)
        #
        # for index,url in enumerate(urls['url']):
        #     print (urls['url'][index])

    # è¨ˆç®—æª”æ¡ˆå¤§å°
    def file_size(self,info):
        if not os.path.exists(self.ori_path + '/' + 'dannyTumblr' + '/' + 'Download' + '/' + info):
            os.makedirs(self.ori_path + '/' + 'dannyTumblr' + '/' + 'Download' + '/' + info)
        os.chdir(self.ori_path + '/' + 'dannyTumblr' + '/' + 'Download' + '/' + info + '/' )

        total_size = 0
        for dirpath, dirname,filename in os.walk(os.getcwd()):
            # print(filename)
            for f in filename:
                single = os.path.join(dirpath,f)
                #print('file path :{}'.format(single))
                total_size += os.path.getsize(single)
        dl_size = round(total_size/(1000*1000),1)  # MB
        # print('{} MB '.format(dl_size))
        os.chdir(self.ori_path)
        return float(dl_size)

